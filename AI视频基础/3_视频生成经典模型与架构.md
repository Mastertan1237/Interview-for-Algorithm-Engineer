# 目录

## 第一章 Stable Video Diffusion模型核心基础知识
- [1.请介绍下StableVideoDiffusion的核心架构？以及有哪些创新点？](#1.请介绍下StableVideoDiffusion的核心架构？以及有哪些创新点？)
- [2.SVD 的架构基础是什么？它如何提高计算效率？](#2.SVD的架构基础是什么？它如何提高计算效率？)
- [3.在从图像模型扩展到视频模型时，SVD这类模型是如何处理时间维度的？](#3.在从图像模型扩展到视频模型时，SVD这类模型是如何处理时间维度的？)
- [4.SVD在进行“图生视频(I2V)”任务时，是如何注入图像条件的？](#4.SVD在进行“图生视频(I2V)”任务时，是如何注入图像条件的？)
- [5.请描述SVD或类似视频模型的典型训练策略。](#5.请描述SVD或类似视频模型的典型训练策略。)
- [6.评估SVD生成视频质量时，常用的自动化指标有哪些？](#6.评估SVD生成视频质量时，常用的自动化指标有哪些？)


## 第二章 Sora模型核心基础知识



## 第三章 字节Seedance模型核心基础知识


## 第四章 快手可灵AI模型核心基础知识


## 第五章 通义Wan模型核心基础知识


## 第六章 腾讯HunyuanVideo模型核心基础知识


## 第七章 智谱CogVideoX模型核心基础知识


---

## 第一章 Stable Video Diffusion模型核心基础知识

<h2 id="1.请介绍下StableVideoDiffusion的核心架构？以及有哪些创新点？">1.请介绍下StableVideoDiffusion的核心架构？以及有哪些创新点？</h2>

Stable Video Diffusion (SVD) 是由 Blattmann 等人（2023a）提出的一种开源基础视频生成模型。它是在潜空间视频扩散模型（Latent Video Diffusion Models, LDM）的基础上，通过在大型数据集上进行扩展训练而成的。

**核心架构**
- 基础架构： SVD 建立在预训练的 Stable Diffusion 2.1 (SD 2.1) 2D 图像生成模型之上，利用其强大的视觉表征能力。
- 潜空间操作： 与直接在像素空间操作的模型不同，SVD 在变分自编码器（VAE）压缩后的低维潜空间内进行训练和去噪，显著降低了计算复杂性，从而能够生成更高分辨率的视频。
- 时空层扩展： 在原有的 2D 空间卷积和注意力块之后，SVD 插入了专门的 1D 时间卷积层和时间注意力层。
- 参数规模： 模型总参数量约为 15.21 亿，其中约 6.56 亿是新加入的时间模块参数。
- 连续时间框架： 采用 EDM 框架（连续时间扩散模型），并对噪声调度（Noise Schedule）进行了优化，以适应高分辨率视频生成的特殊需求。

**核心创新点**
SVD 的成功不仅在于架构，更在于其系统化的训练和数据处理范式：
- 三阶段训练策略： 确定了视频生成模型成功的三个关键阶段：
    1. 阶段 I（图像预训练）： 基于 SD 2.1 建立空间视觉基础。
    2. 阶段 II（视频预训练）： 在大规模视频数据集上以低分辨率进行预训练，学习通用的运动表征。
    3. 阶段 III（高质量微调）： 在精选的高质量、高美感视频子集上进行高分辨率微调。
- 系统化的数据策展（Data Curation）：
    ◦ 级联剪辑检测（Cascaded Cut Detection）： 采用多帧率、多阈值的检测器，识别并过滤掉原始视频中的转场和淡入淡出，确保运动的连续性。
    ◦ 自动化标注： 结合 CoCa（空间细节）、V-BLIP（时间细节）和 LLM 总结，生成极其详尽的合成描述词。
    ◦ 多维度过滤： 利用稠密光流剔除静态镜头，利用 OCR 过滤文字遮挡，并利用 CLIP 审美分确保视觉美感。
- 灵活的条件控制机制：
    ◦ 微调节（Micro-conditioning）： 推理时可输入 FPS（帧率）运动分值，实现对视频动态程度的精确控制。
    ◦ 线性增加引导（Linearly Increasing Guidance）： 在图生视频（I2V）推理中，随时间轴线性增加引导强度，解决了传统固定引导值导致的帧间不一致或饱和度异常问题。
- 强大的 3D 先验能力： SVD 证明了视频扩散模型具备卓越的 3D 理解力。通过简单的微调，SVD 可以转化为多视角生成模型（SVD-MV），以极低的计算开销生成高度一致的物体多视角图像


